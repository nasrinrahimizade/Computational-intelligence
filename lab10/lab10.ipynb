{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOONLUJvMs7xLC3MpYUWDTY"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import random\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "I3hL1CIlfKDP"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "AGENT = 1\n",
        "OPPONENT = -1\n",
        "NO_PLAYER = 0"
      ],
      "metadata": {
        "id": "-rv6ELdGellH"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Game Model\n",
        "class TicTacToe:\n",
        "    def __init__(self):\n",
        "        # Initialize the game board as a 3x3 array\n",
        "        self.board = np.zeros((3, 3))\n",
        "        # Represent the current player (1 for AGENT, -1 for OPPONENT)\n",
        "        self.current_player = AGENT\n",
        "\n",
        "    def make_move(self, move):\n",
        "        # Make a move on the game board\n",
        "        row, col = move\n",
        "        self.board[row, col] = self.current_player\n",
        "        # Switch to the next player\n",
        "        self.current_player *= -1\n",
        "\n",
        "    def valid_moves(self):\n",
        "        # Get a list of valid moves (empty cells) on the current board\n",
        "        return [(i, j) for i in range(3) for j in range(3) if self.board[i, j] == NO_PLAYER]\n",
        "\n",
        "    def is_finished(self):\n",
        "        # Check if the game is finished (either a player wins or it's a draw)\n",
        "        return self.get_winner() != NO_PLAYER or np.all(self.board != NO_PLAYER)\n",
        "\n",
        "    def get_winner(self):\n",
        "        # Check and return the winner of the game\n",
        "        for i in range(3):\n",
        "            if np.all(self.board[i, :] == AGENT) or np.all(self.board[:, i] == AGENT):\n",
        "                return AGENT\n",
        "            if np.all(self.board[i, :] == OPPONENT) or np.all(self.board[:, i] == OPPONENT):\n",
        "                return OPPONENT\n",
        "\n",
        "        if np.all(np.diag(self.board) == AGENT) or np.all(np.diag(np.fliplr(self.board)) == AGENT):\n",
        "            return AGENT\n",
        "        if np.all(np.diag(self.board) == OPPONENT) or np.all(np.diag(np.fliplr(self.board)) == OPPONENT):\n",
        "            return OPPONENT\n",
        "\n",
        "        return NO_PLAYER\n"
      ],
      "metadata": {
        "id": "f3sP0CROfP9k"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Agent with Reinforcement Learning\n",
        "class RLAgent:\n",
        "    def __init__(self, epsilon=0.1, alpha=0.8):\n",
        "        # Initialize the agent with exploration rate (epsilon) and learning rate (alpha)\n",
        "        self.epsilon = epsilon\n",
        "        self.alpha = alpha\n",
        "        # Dictionary to store state values (Q-values)\n",
        "        self.values = {}\n",
        "        # Track the current state during gameplay\n",
        "        self.current_state = None\n",
        "\n",
        "    def choose_action(self, env):\n",
        "        # Choose an action based on epsilon-greedy strategy\n",
        "        if random.uniform(0, 1) < self.epsilon:\n",
        "            return random.choice(env.valid_moves())\n",
        "        else:\n",
        "            return max(env.valid_moves(), key=lambda move: self.get_state_value(env.make_move(move)))\n",
        "\n",
        "    def get_state_value(self, state):\n",
        "        # Get the value of a given state\n",
        "        state_str = str(state)\n",
        "        if state_str not in self.values:\n",
        "            return 0.5  # Default value for unseen states\n",
        "        return self.values[state_str]\n",
        "\n",
        "    def update_state_value(self, state, reward):\n",
        "        # Update the value of a given state using temporal difference\n",
        "        state_str = str(state)\n",
        "        if state_str not in self.values:\n",
        "            self.values[state_str] = 0.5  # Default value for unseen states\n",
        "        self.values[state_str] = self.values[state_str] + self.alpha * (reward - self.values[state_str])\n",
        "\n",
        "    def play(self, env):\n",
        "        # Play a game using the RL agent\n",
        "        self.current_state = None\n",
        "        while not env.is_finished():\n",
        "            action = self.choose_action(env)\n",
        "            self.current_state = env.make_move(action)\n",
        "            env.make_move(action)\n",
        "        reward = env.get_winner()\n",
        "        self.update_state_value(self.current_state, reward)\n"
      ],
      "metadata": {
        "id": "QR1LQx73fWAf"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training function\n",
        "def train_agent(agent, num_episodes=1000):\n",
        "    # Train the agent for a specified number of episodes\n",
        "    for _ in range(num_episodes):\n",
        "        game = TicTacToe()\n",
        "        while not game.is_finished():\n",
        "            agent.play(game)\n",
        "\n",
        "# Evaluation function\n",
        "def evaluate_agent(agent, num_games=1000):\n",
        "    # Evaluate the agent's performance against random moves\n",
        "    wins = 0\n",
        "    draws = 0\n",
        "    for _ in range(num_games):\n",
        "        game = TicTacToe()\n",
        "        while not game.is_finished():\n",
        "            if game.current_player == AGENT:\n",
        "                action = agent.choose_action(game)\n",
        "                game.make_move(action)\n",
        "            else:\n",
        "                action = random.choice(game.valid_moves())\n",
        "                game.make_move(action)\n",
        "        winner = game.get_winner()\n",
        "        if winner == AGENT:\n",
        "            wins += 1\n",
        "        elif winner == NO_PLAYER:\n",
        "            draws += 1\n",
        "    return wins, draws\n"
      ],
      "metadata": {
        "id": "AKHetL16fZi_"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "if __name__ == '__main__':\n",
        "    # Create an instance of RLAgent\n",
        "    rl_agent = RLAgent()\n",
        "    # Train the agent for a specified number of episodes\n",
        "    train_agent(rl_agent, num_episodes=50000)\n",
        "\n",
        "    # Evaluate the agent's performance against random moves\n",
        "    wins, draws = evaluate_agent(rl_agent, num_games=1000)\n",
        "    print(\"Agent Wins: {}%, Draws: {}%\".format(wins * 100 / 1000, draws * 100 / 1000))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ESKQWQqVfcwE",
        "outputId": "0a159d56-905d-4661-d5e9-0da4e54c6c7a"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Agent Wins: 92.8%, Draws: 0.6%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4LUK9STFff9A"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}